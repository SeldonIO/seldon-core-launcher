{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example First Deployment of a Model on Seldon\n",
    "\n",
    "This demo will show you how to describe runtime inference graphs to deploy on seldon-core. We will take advantage of pre-wrapped dummy models that are ready to be managed by seldon-core. The [next notebook](https://github.com/SeldonIO/seldon-core-launcher/blob/master/seldon-core/getting_started/wrap-model/wrap_model.ipynb) will show you how to wrap your own runtime models for seldon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequistes\n",
    "\n",
    " * You have a running cluster installed via the Google Marketplace with all the defaults including:\n",
    "    * NodePort for the Seldon API OAuth Gateway. This gateway is used to connect your business apps to your running models via REST and gRPC.\n",
    "    * The cluster is running in the default namespace\n",
    " * You have kubectl installed and authenticated with the cluster\n",
    " \n",
    " You will need to install some software for this demo:\n",
    " \n",
    " \n",
    " * Install the [requests library](http://docs.python-requests.org/en/master/) to allow you to make REST calls to the Seldon API gateway.\n",
    " * Install [python grpc tools](https://grpc.io/docs/quickstart/python.html) to allow you to make gRPC calls to the Seldon API gateway.\n",
    " * Install [graphviz](https://pypi.org/project/graphviz/) a package to display graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up REST and gRPC methods\n",
    "\n",
    "**Ensure you port forward the seldon api-server REST and GRPC ports**, do this in separate terminals outside the notebook:\n",
    "\n",
    "REST:\n",
    "```\n",
    "kubectl port-forward $(kubectl get pods -l app=seldon-apiserver-container-app -o jsonpath='{.items[0].metadata.name}') 8002:8080\n",
    "```\n",
    "\n",
    "GRPC:\n",
    "```\n",
    "kubectl port-forward $(kubectl get pods -l app=seldon-apiserver-container-app -o jsonpath='{.items[0].metadata.name}') 8003:5000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from proto import prediction_pb2\n",
    "from proto import prediction_pb2_grpc\n",
    "import grpc\n",
    "try:\n",
    "    from commands import getoutput # python 2\n",
    "except ImportError:\n",
    "    from subprocess import getoutput # python 3\n",
    "\n",
    "API_HTTP=\"localhost:8002\"\n",
    "API_GRPC=\"localhost:8003\"\n",
    "\n",
    "def get_token():\n",
    "    payload = {'grant_type': 'client_credentials'}\n",
    "    response = requests.post(\n",
    "                \"http://\"+API_HTTP+\"/oauth/token\",\n",
    "                auth=HTTPBasicAuth('oauth-key', 'oauth-secret'),\n",
    "                data=payload)\n",
    "    print(response.text)\n",
    "    token =  response.json()[\"access_token\"]\n",
    "    return token\n",
    "\n",
    "def rest_request():\n",
    "    token = get_token()\n",
    "    headers = {'Authorization': 'Bearer '+token}\n",
    "    payload = {\"data\":{\"names\":[\"a\",\"b\"],\"tensor\":{\"shape\":[2,2],\"values\":[0,0,1,1]}}}\n",
    "    response = requests.post(\n",
    "                \"http://\"+API_HTTP+\"/api/v0.1/predictions\",\n",
    "                headers=headers,\n",
    "                json=payload)\n",
    "    print(response.text)\n",
    "    \n",
    "def grpc_request():\n",
    "    token = get_token()\n",
    "    datadef = prediction_pb2.DefaultData(\n",
    "            names = [\"a\",\"b\"],\n",
    "            tensor = prediction_pb2.Tensor(\n",
    "                shape = [3,2],\n",
    "                values = [1.0,1.0,2.0,3.0,4.0,5.0]\n",
    "                )\n",
    "            )\n",
    "    request = prediction_pb2.SeldonMessage(data = datadef)\n",
    "    channel = grpc.insecure_channel(API_GRPC)\n",
    "    stub = prediction_pb2_grpc.SeldonStub(channel)\n",
    "    metadata = [('oauth_token', token)]\n",
    "    response = stub.Predict(request=request,metadata=metadata)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example Model\n",
    "A simple example is shown below we use a single prepacked model for illustration. The spec contains a set of predictors each of which contains a ***componentSpecs*** which is a list of Kubernetes [PodTemplateSpec](https://kubernetes.io/docs/api-reference/v1.9/#podtemplatespec-v1-core) alongside a ***graph*** which describes how components fit together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizer import get_graph\n",
    "get_graph(\"model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple graph we will deploy has a single model \"classifier\" which is exposed via REST. The actual JSON for this representation is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./model.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note your PodTemplateSpec inside componentSpecs has in this case kubernetes resource requests for 1 Mb of memory. You can also add volumes, and other requirements for your model to run successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Seldon Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the runtime graph to kubernetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f ./model.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get seldondeployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the status of the SeldonDeployment. **When ready the replicasAvailable should be 1 for all components**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get seldondeployments seldon-deployment-example -o jsonpath='{.status}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REST Request\n",
    "We will get an OAuth token using the key and secret we specified in the graph above and then call the REST endpoint of the API gateway with some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_request()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gRPC Request\n",
    "We will an OAuth token using the key and secret we specified in the graph above and then call the gRPC endpoint of the API gateway with some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpc_request()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update deployment to AB test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will change the deployment to add an AB test deployment. This illustrates:\n",
    "\n",
    " * Updating a deployment with no downtime.\n",
    " * Creating a more complex runtime graph.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_graph(\"random_ab_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above we now have 2 models both serving REST requests and we have attached them behind a builtin component of Seldon which provides a random AB test router to split traffic between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./random_ab_test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f random_ab_test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the deployments. Note: **Might need to run several times until replicasAvailable is 1 for both predictors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get seldondeployments seldon-deployment-example -o jsonpath='{.status}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REST Request\n",
    "We should not have updated our deployment with the new model while the previous one will still be running. We can test again using the REST API. Note the \"routing\" meta data returned will show which route our request travelled, either \"0\" or \"1\" for the two runtime graphs we have created with separate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_request()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gRPC request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpc_request()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tear Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f random_ab_test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations, you have deployed a model and interacted with it over REST and gRPC. You have then deployed an AB test between two models to update your runtime graph with no downtime.\n",
    "\n",
    "For the next steps you will want to [wrap your own machine learning models to deploy onto seldon-core](https://github.com/SeldonIO/seldon-core-launcher/blob/master/seldon-core/getting_started/wrap-model/wrap_model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
